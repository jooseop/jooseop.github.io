---
title:  "k8s 13. 로깅 & 모니터링"
excerpt:

categories:
  - k8s

toc: true
toc_sticky: true
 
date: 2021-10-14
---

# 1. 로깅

## 1-1. 로그

* 로그 : 문제를 디버깅하고 클러스터 활동을 모니터링하는데 유용

```shell
$ kubectl logs -f 파드이름
```

> 컨테이너 로그 - 로그 에이전트(fluentd) - 로그 저장(Elasticsearch) - 로그 시각화(kibana)
>
> \+ 실시간 모니터링(stern)



## 1-2. ELK Stack

### 개념

* 모든 시스템과 애플리케이션에서 로그를 집계하고 이를 분석하며 애플리케이션과 인프라 모니터링 시각화를 생성하고, 빠르게 문제를 해결하며 보안 분석할 수 있는 능력을 제공
* Elasticsearch : 검색 및 분석 엔진 -> 수집한 로그 저장
* Kibana : 사용자가 Elasticsearch에서 차트와 그래프를 이용해 데이터를 시각화 -> 시각화
* Logstash : 여러 소스에서 동시에 데이터를 수집하여 변환한 후 Elasticsearch 같은 “stash”로 전송하는 서버 사이드 데이터 처리 파이프라인

### Elasticsearch: 로그 저장

* 수집한 로그 저장

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: elasticsearch
  labels:
    app: elasticsearch
spec:
  replicas: 1
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      #-------------------------------------
      containers:
      - name: elasticsearch
        image: elastic/elasticsearch:6.4.0
        env:
        - name: discovery.type
          value: "single-node"
        ports:
        - containerPort: 9200
        - containerPort: 9300
      #-------------------------------------
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: elasticsearch
  name: elasticsearch-svc
  namespace: default
spec:
  type: NodePort
  selector:
    app: elasticsearch
  #-------------------------------------
  ports:
  - name: elasticsearch-rest
    nodePort: 30920
    port: 9200
    protocol: TCP
    targetPort: 9200
  - name: elasticsearch-nodecom
    nodePort: 30930
    port: 9300
    protocol: TCP
    targetPort: 9300  
  #-------------------------------------
```

```shell
# 엔드포인트 확인
$ kubectl get endpoints
$ curl <endpoint>:9200
{
  "name" : "QCNV78N",
  "cluster_name" : "docker-cluster",
  "cluster_uuid" : "4Ifplq3dTySn4zPCaBkNcw",
  "version" : {
    "number" : "6.4.0",
    "build_flavor" : "default",
    "build_type" : "tar",
    "build_hash" : "595516e",
    "build_date" : "2018-08-17T23:18:47.308994Z",
    "build_snapshot" : false,
    "lucene_version" : "7.4.0",
    "minimum_wire_compatibility_version" : "5.6.0",
    "minimum_index_compatibility_version" : "5.0.0"
  },
  "tagline" : "You Know, for Search"
}
```

### Kibana: 로그 데이터 시각화

* 데이터 시각화 도구

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  labels:
    app: kibana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      containers:
      - name: kibana
        image: elastic/kibana:6.4.0
        #-----------------------------------------------------------------
        env:
        - name: SERVER_NAME
          value: "kibana.kubenetes.example.com"
        - name: ELASTICSEARCH_URL
          value: "http://elasticsearch-svc.default.svc.cluster.local:9200"
          # >>>>>>>>>>>> ---.namespace.svc.cluster.local <<<<<<<<<<<<<<<<
        ports:
        - containerPort: 5601
        #-----------------------------------------------------------------
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: kibana
  name: kibana-svc
  namespace: default
spec:
	type: NodePort
	selector:
    app: kibana
  #-------------------------------------
  ports:
  - nodePort: 30561
    port: 5601
    protocol: TCP
    targetPort: 5601
  #-------------------------------------
```



## 1-3. 클러스터 레벨 로깅

* 클러스터에서 로그는 노드, 파드 또는 컨테이너와는 독립적으로 별도의 스토리지와 라이프사이클을 가져야 한다. 왜냐하면 컨테이너가 종료되거나, 노드에 장애가 있더라도 앱 컨테이너의 로그를 확인할 수 있어야 되기 때문이다. 쿠버네티스의 생명 주기와 분리된 스토리지를 구축하는 아키텍처, 이 개념을 클러스터-레벨-로깅 이라고 한다.

### 컨테이너 로그

![image](https://user-images.githubusercontent.com/65662520/137356069-f8b00a8f-b463-4a61-8acc-5c324c92ca1e.png)

* 컨테이너의 로그 수집

  1. 수집 : 컨테이너 런타임(도커)
  2. 출력 : stdout(표준출력), stderr(표준에러) 스트림에 의한 출력 -> log-file.log(로깅 드라이버)
  3. 로깅 드라이버 : json-file(기본), syslog, journal
  4. 로그 로테이션 : logrotate, 로그가 노드에서 사용 가능한 모든 스토리지를 사용하지 않도록 하는 것
* 컨테이너 로그
  * 로그 : 로그 내용 ("log"), 표준 스트림 종류("stream":"stderr"), RFC3339Nano 시간정보("time")
  * 도커 로그 설정 확인 : `docker inspect containerID`
* kubelet : 로그를 로테이션, 로깅 디렉터리 구조 관리 (컨테이너 로그 파일에 대해 심볼릭 링크를 생성하여 관리)
  * 로그 파일 : `/var/lib/docker/containers/컨테이너ID/컨테이너ID-json.log`
  * 심볼릭 링크
    * `/var/log/continers/[파드이름]_[파드네임스페이스]_[컨테이너이름][컨테이너ID].log`
    * `/var/log/pods/파드UID/컨테이너이름/0.log`

### 시스템 컴포넌트 로그

* 쿠버네티스에서 컨테이너 기반으로 동작하지 않는 시스템 구성요소(kubelet, kube-proxy, container 등)에 대한 로그
* systemd를 사용하는 시스템에서는, kubelet과 컨테이너 런타임은 journald에 작성한다.
* systemd를 사용하지 않으면, kubelet과 컨테이너 런타임은 `/var/log` 디렉터리의 `.log` 파일에 작성한다. 

### 클러스터 레벨 로깅 아키텍처

![image](https://user-images.githubusercontent.com/65662520/137409417-63388ae0-491c-494d-89d6-4371f802bbac.png)

* 생성된 로그를 logging-agent를 통해 백엔드로 푸시한다.
* 로깅 에이전트
  * 해당 노드의 모든 애플리케이션 컨테이너에서 로그 파일이 있는 디렉터리에 접근할 수 있는 컨테이너
  * 모든 노드에서 실행해야 하므로, 에이전트는 `DaemonSet` 으로 동작
  * 노드별 하나의 에이전트만 생성



## 1-4. fluentd: 로그 에이전트

### 개념

* 로깅 에이전트, open-source data collector
* 로그 수집 -> 백엔드(일레스틱서치)에 저장
* 루비기반, 플러그인 사용 가능

### 템플릿

* https://github.com/fluent/fluentd-kubernetes-daemonset

* fluentd-daemonset-elasticsearch.yaml

  ```yaml
  
  apiVersion: apps/v1
  kind: DaemonSet # <<<<<<<<<<<-----------
  metadata:
    name: fluentd
    namespace: kube-system # <<<<<<<<<<<-----------
    labels:
      k8s-app: fluentd-logging
      version: v1
  spec:
    selector:
      matchLabels:
        k8s-app: fluentd-logging
        version: v1
    template:
      metadata:
        labels:
          k8s-app: fluentd-logging
          version: v1
      # --------------------------------------------------------------------------
      spec:
        # 서비스
      	serviceAccount: fluentd
  			serviceAccountName: fluentd
  			terminationGracePeriodSeconds: 30
  			
  			# 톨러레이션
        tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
        
        # ===================================================================================
        # 컨테이너
        containers:
        - name: fluentd
          image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
          # ====================================================
          # env
          env:
            - name:  FLUENT_ELASTICSEARCH_HOST
              value: "elasticsearch-logging"
            - name:  FLUENT_ELASTICSEARCH_PORT
              value: "9200"
            - name: FLUENT_ELASTICSEARCH_SCHEME
              value: "http"
              
            # self signed certs
            - name: FLUENT_ELASTICSEARCH_SSL_VERIFY
              value: "true"
              
            # TLS 
            - name: FLUENT_ELASTICSEARCH_SSL_VERSION
              value: "TLSv1_2"
  
            # X-Pack Authentication ====================
            - name: FLUENT_ELASTICSEARCH_USER
              value: "elastic"
            - name: FLUENT_ELASTICSEARCH_PASSWORD
              value: "changeme"
              
            # Logz.io Authentication ====================
            - name: LOGZIO_TOKEN
              value: "ThisIsASuperLongToken"
            - name: LOGZIO_LOGTYPE
              value: "kubernetes"
          # ====================================================
          # 리소스
          resources:
            limits:
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 200Mi
          # 볼륨 마운트
          volumeMounts:
          - name: varlog
            mountPath: /var/log
          - name: dockercontainerlogdirectory
            mountPath: /var/log/pods
            readOnly: true
          # - name: dockercontainerlogdirectory
          #   mountPath: /var/lib/docker/containers
          #   readOnly: true
        # ===================================================================================
        # 볼륨
        volumes:
        - name: varlog
          hostPath:
            path: /var/log
        - name: dockercontainerlogdirectory
          hostPath:
            path: /var/log/pods
        # - name: dockercontainerlogdirectory
        #   hostPath:
        #     path: /var/lib/docker/containers
  ```

### 실행

* elasticsearch 실행 -> fluentd 실행

```shell
# fluentd 생성한 파드 확인
$ kubectl get pods -n kube-system

# 파드 접속
$ kubectl execc -it 파드이름 -n kube-system -- sh

# 로그 확인
ls -alF /var/log
ls -alF /var/lib/docker/containers/
```

* /var/lib/docker/containers : 실제 컨테이너들이 데이터를 저장, fluentd 컨테이너에 이 디렉터리가 마운트되어 로그 데이터 수집
* 이후, kibana를 사용하여 시각화된 데이터 조회



## 1-5. stern: 로그 모니터링

* shell에서 바로 여러 개의 파드 로그를 실시간으로 모니터링

```shell
$ stern kube -n kube-system
-n : 네임세스템 // --all-namespaces
-l : 레이블
-o : 출력 방식 // -o json
```



# 2. 대시보드

## 개념

* CLI인 kubectl 대신 웹UI 기반인 쿠버네티스 대시보드를 사용하여 쿠버네티스를 관리할 수 있다.
* 깃허브 : https://github.com/kubernetes/dashboard
* 쿠버네티스 문서 : https://kubernetes.io/ko/docs/tasks/access-application-cluster/web-ui-dashboard/
* 샘플사용자 만들기 : https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md

## 설치

1. 다운로드

```shell
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml
```

2. recommended 수정
   * [샘플 사용자 만들기](https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md) 참고
   * Service Account, ClusterRoleBinding 추가
   * Service - kubernetes-dashboard : {type: NodePort, nodePort: 30443} 추가
3. 실행 후 접속

4. 로그인 토큰

   ```shell
   kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"
   
   # 토큰 사용 하여 접속
   ```

# 3. 모니터링

![image](https://user-images.githubusercontent.com/65662520/137442564-2d35185c-1bec-4336-996f-5902f4ab6be6.png)

## 3-1. 모니터링 아키텍처

### 메트릭

* 메트릭 : 주어진 노드나 파드에서 현재 사용중인 리소스 (컨테이너 CPU 및 메모리) 사용량
* 시스템 메트릭 : 쿠버네티스 리소스 사용량
  * 코어 메트릭 : 쿠버네티스가 직접 사용
  * non-코어 메트릭 : 다른 시스템 메트릭
* 서비스 메트릭 : 애플리케이션 모니터링을 위한 리소스 사용량
  * 쿠버네티스 인프라용 컨테이너 메트릭
  * 사용자 애플리케이션 메트릭

### 파이프라인

* 파이프라인 : 컴퓨터 과학에서 파이프라인은 한 데이터 처리 단계의 출력이 다음 단계의 입력으로 이어지는 형태로 연결된 구조를 가리킨다. 이렇게 연결된 데이터 처리 단계는 한 여러 단계가 서로 동시에, 또는 병렬적으로 수행될 수 있어 효율성의 향상을 꾀할 수 있다. (https://ko.wikipedia.org/wiki/파이프라인_(컴퓨팅))
* 코어 메트릭 파이프라인
  * 시스템 메트릭 수집
  * 쿠버네티스 관련 구성 요소가 직접 관리, 핵심요소 모니터링 - kubelet, 메트릭 서버, 메트릭 API, cAdvisor
  * 시스템 컴포넌트, 스케줄러, HPA 작업시 사용
* 모니터링 파이프라인
  * 시스템 메트릭 + 서비스 메트릭 수집
  *  외부 모니터링 시스템과 연계, 코어 메트릭 파이프라인 + 모니터링 파이프라인
  * cAdvisor + 프로메테우스

## 3-2. 메트릭 서버

### 개념

* 클러스터 전역에서 리소스 사용량 데이터를 집계
* 쿠버네티스 클러스터의 상태를 모니터링하기 위한 용도로 사용
* kubelet으로 수집한 메트릭 데이터를 kube-apiserver로 조회하는 메트릭 API 방식으로 제공
* 모니터링 표준

### 설치

* https://github.com/kubernetes-sigs/metrics-server/releases
* components.yaml
* .spec.template.spec.containers.args : --kubelet-insecure-tls 추가
  * 사용자 정의 인증서, 보안 에러 발생하지 않게 무시하는 옵션

### 사용

```shell
$ kubectl top node
$ kubectl top pod

# 1. 메트릭 서버 요청을 위한 포트포워딩
$ kubectl port-forward svc/metrics-server -n kube-system 30443:443

# 2. 메트릭 API - 쿠커네티스 API를 이용하여 인증
$ TOKEN=$(kubectl describe secret $(kubectl get secrets | grep default | cut -f1 -d ' ') | grep -E '^token' | cut -f2 -d':' | tr -d ' ')
$ kubectl get secrets # 토큰 정보 확인

# 3. API 목록 확인
$ curl -k -H "Authorization: Bearer $TOKEN" https://localhost:30443/

# 4. 메트릭 API 정보 확인
$ curl -k -H "Authorization: Bearer $TOKEN" https://localhost:30443/metrics

# 5. 종료ㅣ
```



## 3-3. 프로메테우스: 모니터링

### 기능

* 실시간으로 메트릭을 수집, 모니터링 오픈소스
* 웹 ui, 그라파나와 연계하여 시각화
  * 그라파나 : 모니터링 시각화 대시보드 오픈소스
* 프로메테우스 서버가 수집하려는 대상에서 데이터를 가져오는 풀 구조
* 외부에서 직접 푸시한 모니터링 데이터를 푸시 게이트웨이로 받아서 저장
* 시계열 데이터를 저장할수 있는 다차원 데이터모델과 데이터 모델을 효과적으로 활용할 수 있는 PromQL이라는 쿼리 언어 사용

### 구성

#### 컴포넌트

* 프로메테우스 서버 : 시계열 데이터 수집, 저장
* 클라이언트 라이브러리 : 애플리케이션 개발시, 프로메테우스에서 데이터를 수집하도록 만드는 라이브러리
* push gateway : 클라이언트에서 직접 프로메테우스로 데이터를 보낼 때 받는 역할
* exporter : 프로메테우스 클라이언트 라이브러리가 없는 애플리케이션에서 데이터를 수집할 때 사용
* alertmanager : 알람 처리, 관리 (중복처리, 그룹화)

#### 프로메테우스 아키텍쳐

![image](https://user-images.githubusercontent.com/65662520/137454326-635399f7-2995-4932-bd58-fda49fa952fa.png)

### 사용

1. 프로메테우스 configMap

   * https://github.com/prometheus/prometheus/blob/main/documentation/examples/prometheus-kubernetes.yml

   * 파일이름 변경 : prometheus-kubernetes-config.yaml

   * 추가

     ```yaml
     scrape_configs:
       - job_name: "kubernetes-apiservers"
       ...
     
       - job_name: "kubernetes-nodes"
         scheme: https
         tls_config:
           ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
         authorization:
           credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
         kubernetes_sd_configs:
           - role: node
         relabel_configs:
           - action: labelmap
             regex: __meta_kubernetes_node_label_(.+)
           # 추가 ------------------------------------------
           - target_label: __address__
           	replacement: kubernetes.default.svc:443
           - source_labels: [__meta_kubernetes_node_name]
             regex: (.+)
             target_label: __metrics_path__
             replacement: /api/v1/nodes/${1}/proxy/metrics
           # ----------------------------------------------
     
       - job_name: "kubernetes-cadvisor"
         scheme: https
         metrics_path: /metrics/cadvisor
         tls_config:
           ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
         authorization:
           credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
         kubernetes_sd_configs:
           - role: node
         relabel_configs:
           - action: labelmap
             regex: __meta_kubernetes_node_label_(.+)
           # 추가 ------------------------------------------
           - target_label: __address__
           	replacement: kubernetes.default.svc:443
           - source_labels: [__meta_kubernetes_node_name]
             regex: (.+)
             target_label: __metrics_path__
             replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
           # ----------------------------------------------
     
       - job_name: "kubernetes-service-endpoints"
       ...
     
       - job_name: "kubernetes-services"
     	...
     	
       - job_name: "kubernetes-ingresses"
     	...
       
       - job_name: "kubernetes-pods"
     	...
     ```

   * 컨피그맵 생성

     ```shell
     $ kubectl create configmap prometheus-kubernetes --from-file=./prometheus-kubernetes-config.yaml
     ```

2. 프로메테우스 디플로이먼트, 서비스

   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: prometheus-app
     labels:
       app: prometheus-app
   spec:
     replicas: 1
     selector:
       matchLabels:
         app: prometheus-app
     template:
       metadata:
         labels:
           app: prometheus-app
       spec:
         # ------------------------------------------------------------------
         containers:
         - name: prometheus-app
           image: prom/prometheus:v2.3.2
           args:
           - "--config.file=/etc/prometheus/prometheus-kubernetes-config.yaml"
           ports:
           - containerPort: 9090
           # ==============================
           volumeMounts:
           - name: config-volume
             mountPath: /etc/prometheus
           - name: storage-volume
             mountPath: /prometheus/
         # ------------------------------------------------------------------
         volumes:
         - name: config-volume
           configMap:
             name: prometheus-kubernetes
         - name: storage-volume
           emptyDir: {}
         # ------------------------------------------------------------------
   ---          
   apiVersion: v1
   kind: Service
   metadata:
     labels:
       app: prometheus-app
     name: prometheus-app-svc
     namespace: default
   spec:
     ports:
     - nodePort: 30990                         # ***** NodePort *****
       port: 9090
       protocol: TCP
       targetPort: 9090
     selector:
       app: prometheus-app
     type: NodePort
   ```

3. 프로메테우스 & 그라파나 연동

   ```yaml
   apiVersion: extensions/v1beta1
   kind: Deployment
   metadata:
     name: grafana-app
   spec:
     replicas: 1
     template:
       metadata:
         labels:
           k8s-app: grafana
       spec:
         containers:
         - name: grafana
           image: grafana/grafana:5.2.3        # ***** 그라파나 이미지 *****
           ports:
           - containerPort: 3000
             protocol: TCP
           # -----------------------------------------------------------
           env:
           - name: GF_SERVER_HTTP_PORT         # 그라파나 포트
             value: "3000"
           - name: GF_AUTH_BASIC_ENABLED       # 그라파나 인증 사용 안함
             value: "false"
           - name: GF_AUTH_ANONYMOUS_ENABLED   # 익명 사용자 허용
             value: "true"
           - name: GF_AUTH_ANONYMOUS_ORG_ROLE  # 익명 사용자 관리자 권한으로 사용
             value: Admin
           - name: GF_SERVER_ROOT_URL          # 루트 경로
             value: /
           # -----------------------------------------------------------
   ---
   apiVersion: v1
   kind: Service
   metadata:
     labels:
       kubernetes.io/name: grafana-app
     name: grafana-app
   spec:
     ports:
     - port: 3000
       targetPort: 3000
       nodePort: 30300                           # ***** nodePort *****
     selector:
       k8s-app: grafana
     type: NodePort
   ```

4. 그라파나 - 프로메테우스 설정

   1. Add data source
   2. prometheus 추가 : URL - http://prometheus-app-svc.default.svc.cluster.local:9090
   3. 대시보드, 패널 생성
      * 사용자들이 만들어 놓은 대시보드 사용가능
      * import via grafana.com 8588
   4. 데이터 소스 선택
