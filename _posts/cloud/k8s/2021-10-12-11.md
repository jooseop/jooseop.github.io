---
title:  "k8s 11. Storage"
excerpt:

categories:
  - k8s

toc: true
toc_sticky: true
 
date: 2021-10-12
---

# 1. Volume

* 목적
  * 컨테이너는 기본적으로 stateless이기 때문에, 컨테이너를 다시 시작하면 초기화가 되고, 파드간 같이 실행되는 컨테이너 간에 파일을 공유할때 문제가 발생한다. 이러한 점을 해결하기 위해 볼륨을 사용한다.
  * 데이터 유지, 공유
* 설정 :  `.spec.volumes` `.spec.containers[*].volumeMounts`
* 타입
  * 클라우드 서비스 : aws\_, azure_, gce\_
  * 오픈소스 서비스 : glusterfs, cephfs
  * 쿠버네티스 내부 오브젝트 : configmap, secret
  * 컨테이너 : emptyDir, hostPath, local

# 2. Container - volume

## 2-1. emptyDir

### 개념

* 노드에 파드가 할당될 때 처음 생성, 해당 노드에서 파드가 실행되는 동안에만 작동 (파드가 삭제되면 삭제)
* 파드 내 모든 컨테이너가 접근하여 사용한다. 파드 내 컨테이너에서 볼륨 공유

### 설정

```ini
spec:
	containers:
	ㅣ image:
	ㅣ name:
  ㅣ volumeMounts:
  		L mountPath: 마운트 할 위치
  		L name: 볼륨이름
	volumes:
	ㅣ name: 볼륨이름
	##############
	ㅣ emptyDir: {}
	##############
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    ######################
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  ######################
  volumes:
  - name: cache-volume
    emptyDir: {}
  ######################
```

* 확인

  ```shell
  $ kubectl exec <파드이름> -- ls -ld /emptydir
  
  $ kubectl exec <파드이름> -- <명령어> : 파드에서 명령어를 실행
  -l : long, 자세한 출력
  -d : 디렉토리만 출력
  ```



## 2-2. hostPath

### 개념

* docker의 bindMount와 같은 개념
* 호스트에 데이터가 남는다.
  * emptyDir : 임시 디렉터리 마운트
  * hostPath : 호스트에 있는 실제 파일이나 디렉터리를 마운트, 호스트의 중요 디렉터리를 컨테이너에 마운트
    * 도커 내부에 접근할 필요가 있는 실행중인 컨테이너 -> /var/lib/docker
    * 컨테이너에서 cAdvisor 실행 -> /sys
* 보안 위험이 있으므로 사용해야 할 경우 ReadOnly로 마운트한다.

### 설정

* volumes.hostPath.type

  | Value               | Behavior                                                     |
  | ------------------- | ------------------------------------------------------------ |
  | (설정안함)          | 빈 문자열 (기본값)은 이전 버전과의 호환성을 위한 것으로, hostPath 볼륨은 마운트 하기 전에 아무런 검사도 수행되지 않는다. |
  | `DirectoryOrCreate` | 만약 주어진 경로에 아무것도 없다면, 필요에 따라 Kubelet이 가지고 있는 동일한 그룹과 소유권, 권한을 0755로 설정한 빈 디렉터리를 생성한다. |
  | `Directory`         | 주어진 경로에 디렉터리가 있어야 함                           |
  | `FileOrCreate`      | 만약 주어진 경로에 아무것도 없다면, 필요에 따라 Kubelet이 가지고 있는 동일한 그룹과 소유권, 권한을 0644로 설정한 빈 파일을 생성한다. |
  | `File`              | 주어진 경로에 파일이 있어야 함                               |
  | `Socket`            | 주어진 경로에 UNIX 소캣이 있어야 함                          |
  | `CharDevice`        | 주어진 경로에 문자 디바이스가 있어야 함                      |
  | `BlockDevice`       | 주어진 경로에 블록 디바이스가 있어야 함                      |


```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    ######################
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  ########################
  volumes:
  - name: test-volume
    hostPath:
      # 호스트의 디렉터리 위치
      path: /data
      # 이 필드는 선택 사항이다
      type: Directory
  ########################
```



## 2-3. nfs

### 개념

* NFS 서버를 이용해서 파드에 마운트, NFS 클라이언트 역할
* NFS : network file system
* 안정성 높은 외부 스토리지를 볼륨으로 설정하여, 여러 개 파드에서 공유하여 사용
* 고성능보다는 안정성

### 설정

#### 1) NFS 서버 (스토리지)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-server
  labels:
    app: nfs-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nfs-server
  template:
    metadata:
      labels:
        app: nfs-server
    spec:
    	########################
      containers:
      - name: nfs-server
        image:
        ports:
        # 포트1. nfs
        - name: nfs
          containerPort: 2049
        # 포트2. NFS 서버에서 사용하는 프로세스, 요청이 왔을 떄 지정한 디렉터리로 볼륨 마운트하는 mountd 데몬 
        - name: mountd
          containerPort: 20048
        # 포트3. NFS 서버에서 사용하는 프로세스, RPC(remote procedure call)서비스 관리
        - name: rpcbind
          containerPort: 111
        # 컨테이너 보안 설정 : 모든 호스트 장치 접근
        securityContext:
          privileged: true
        # 마운트
        volumeMounts:
        - mountPath: /exports
          name: hostpath-vol
      ########################
      # 볼륨 : hostpath로 설정
      volumes:
      - name: hostpath-vol
        hostPath:
          path: /tmp
          type: Directory
```

* 서버 컨테이너의 IP 확인

  ```shell
  $ kubectl get pods -o widex           
  NAME                              READY   STATUS    RESTARTS   AGE   IP              NODE    NOMINATED NODE   READINESS GATES
  pod/nfs-server-554ccb89ff-fpkwc   1/1     Running   0          80s   10.233.90.125   node1   <none>           <none>
  
  ```

#### 2) NFS 클라이언트 (컨테이너, 파드)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubernetes-nfsapp-pod
  labels:
    app: nfs-client
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nfs-client
  template:
    metadata:
      labels:
        app: nfs-client
    spec:
    	########################
      containers:
      - name: kubernetes-nfsapp-pod
        image:
        volumeMounts:
        - mountPath: /test-nfs
          name: nfs-vol
        ports:
        - containerPort: 8080
      ########################
      # 볼륨 : nfs 서버 파드의 IP를 통해 NFS 볼륨 설정
      volumes:
      - name: nfs-vol
        nfs:
          server: # NFS 서버 파드의 IP #
          path: /root
```

* 클라이언트 파드 확인 (레이블 이용)

  ```shell
  $ kubectl get pods -l <key>=<value>
  ```

* 클라이언트 파드 접속

  ```shell
  $ kubectl exec -it <클라이언트 파드> -- sh
  ```

* 파드에 접속해서 마운트 경로 확인

  ```shell
  # cd <마운트 경로>
  ```

----

```shell
# 서버파드
# 마운트
$ apt-get install nfs-common
$ showmount -e <파드IP>
```



# 3. PersistentVolume

디플로이먼트에서 바로 볼륨을 생성하지 않고, 퍼시스턴트 볼륨으로 마운트하여 안정성을 높인다.

## 3-1. 개념

* PV (PersistentVolume) : 볼륨 자체, 클러스터 리소스, 파드와는 별개로 관리
* PVC (PersistentVolumeClaime)
  * 사용자가 pv에게 하는 요청 (용량, r/w 접근 모드 등)
  * 파드가 노드의 리소스를 사용하는 것처럼, pvc는 pv의 리소스를 사용
  * pvc를 사용하므로써, 파드에 직접 연결하는 것이 아니므로 어떤 스토리지를 사용할지 신경쓰지 않아도 됨
  * 파드에 직접 할당하는 것이 아니라, 중간에 pvc를 두어 파드와 파드가 사용할 스토리지를 분리한 구조에서 다양하게 사용할 수 있도록 함
* PV LifeCycle
  1. 프로비저닝
     * pv 생성
     * 프로비저닝 방법
       1. 정적 방법 : 미리 pv를 생성하여, 요청이 있으면 만들어둔 pv를 할당, 요청할 때 용량에 제한이 있다.
       2. 동적 방법 : pvc를 거쳐 pv를 요청했을 때 생성
  2. Binding
     *  pv를 pvc와 연결
     * 1대1  맵핑, pvc하나가 여러 개 pv에 바인딩 되지 못함
  3. Using
     * 파드에 pvc를 설정하고 볼륨으로 인식하여 사용
     * Storage Object in Use Protection
       * pvc를 사용하는 상태 -> pvc를 사용하는 파드가 존재 -> 앞과 같은 경우일 때 pvc를 삭제 되지 않도록 한다.
  4. Reclaiming
     * 사용 끝난  pvc 삭제, pv 초기화
     * 초기화 정책
       1. Retain : pv를 그대로 보존, 재사용 할 경우 스토리지 자산 정의로 새 pv를 생성
       2. Delete : 외부 스토리지 볼륨 삭제, 동적 프로비저닝 볼륨 기본값
       3. Recycle : pv 데이터 삭제 후 다시 pvc에서  pv를 사용할 수 있도록 함

## 3-2. template

### pv

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
  #--------------------
  #--------------------
  
  
spec:
  #--------------------
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.1
  #--------------------
  nfs:
    path: /tmp
    server: 172.17.0.2
```

* capacity.storage : 용량
* volumeMode : 볼륨모드
  * Filesystem : 파드의 디렉터리에 마운트
  * Block : 파일시스템이 없는 블록 장치로 파드에 제공
* accessModes: 접근모드
  * ReadWriteOnce : 하나의 노드
  * ReadWriteMany : 여러 노드
  * ReadOnlyMany : 여러 노드
  * ReadWriteOncePode : 하나의 파드
* persistentVolumeReclaimPolic : pv reclaim 정책
* storageClassName : 스토리지 클래스
* mountOptions

```shell
# 설정 후 pv 상태 확인
# STATUS -> Available
# 볼륨 Phase : Available, Bound, Released, Failed
$ kubectl get pv
NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   "STATUS"  CLAIM  STORAGECLASS  REASON   AGE
                                                   "Available"
```

### pvc

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 8Gi
  storageClassName: slow
  selector:
    matchLabels:
      release: "stable"
    matchExpressions:
      - {key: environment, operator: In, values: [dev]}
```

* accessModes, volumeMode : 볼륨과 도일
* resuorces.requests.storage : pv를 초과할 수 없음
* storageClassName
* selector
  * `matchLabels` - 볼륨에 이 값의 레이블이 있어야 한다.
  * `matchExpressions` - key, operator, value // operator (In, NotIn, Exists, DoesNotExist)

```yaml
# pv 확인
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   "STATUS"   "CLAIM"  STORAGECLASS   REASON   AGE
                                                     "Bound"    "default/<PVC>"   
# pvc 확인
NAME     "STATUS"     "VOLUME"     CAPACITY     ACCESS MODES     STORAGECLASS     AGE
         "Bound"       "<PV>"
```

### 파드 (pvc를 사용하는 파드)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: task-pv-pod
spec:

  # ----------------------------
  containers:
    - name: task-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      # ----------------------------
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: task-pv-storage
  # ----------------------------
  volumes:
    - name: task-pv-storage
      persistentVolumeClaim:
        claimName: task-pv-claim
```

* volumes.persistentVolumeClaim.claimName : PVC 이름
* containers.volumeMounts
* 확인 : 파드 접속하여 확인



# 4. 프로비저닝

## 4-1. nfs 정적 PV

### 컨트롤 플레인

1. control-plane NFS 서비스 설치

   ```shell
   $ sudo apt install nfs-kernel-server 
   ```

2. NFS 공유 설정

   ```shell
   $ sudo mkdir /nfs-volume
   
   $ sudo vi /etc/exports
   #--------------------------------------
   /nfs-volume *(rw,sync,no_subtree_check)
   #--------------------------------------
   ```

3. NFS 권한 설정

   ```shell
   $ sudo chown -R nobody:nogroup /nfs-volume/
   $ sudo chmod 777 /nfs-volume/
   ```

4. NFS 서비스 시작 및 확인

   ```shell
   $ sudo systemctl restart nfs-kernel-server.service
   $ systemctl status nfs-kernel-server.service 
   ```

5. 방화벽 NFS 허용

   ```shell
   $ sudo iptables -A INPUT -p tcp --dport 2049 -j ACCEPT
   $ sudo iptables -A INPUT -p udp --dport 2049 -j ACCEPT
   ```

### 노드 (파드가 있는 노드)

1. 각 노드 접속

2. NFS 클라이언트 설치

   ```shell
   $ sudo apt-get install nfs-common
   
   # for문 사용시
   $ for i in {1..3}; do ssh kube-node$i sudo apt-get install nfs-common; done
   ```

### pv, pvc, 파드

pv, pvc, 파드 생성

서비스 생성 -> kubectl get endpoint

## 4-2. 동적 프로비저닝

동적 프로비저닝 기능을 사용하면 클러스터 관리자가 스토리지를 사전 프로비저닝 할 필요가 없다. 대신 사용자가 스토리지를 요청하면 자동으로 프로비저닝 한다.

### 준비

1. vagrant 스냅샷

   ```shell
   $ vagrant snapshot save <스냅샷 이름>
   $ vagrant snapshot list
   ```

2. 각 노드마다 디스크 추가

   * VirtualBox 각 노드마다 디스크 추가

3. 디스크 확인

   ```shell
   # ls -l /dev/sd*
   brw-rw---- 1 root disk 8,  0 Oct 13 02:18 /dev/sda
   brw-rw---- 1 root disk 8,  1 Oct 13 02:18 /dev/sda1
   brw-rw---- 1 root disk 8, 16 Oct 13 02:18 /dev/sdb
   brw-rw---- 1 root disk 8, 32 Oct 13 02:18 /dev/sdc
   ```

### rook 스토리지 설치 (control-plane)

* Ceph : 가상스토리지
  * 참고 : https://wcc8088.tistory.com/127
* Rook
  * 가상 솔루션(ceph, cassandra, nfs, edgefs, ...) 배포 도구
  * 오픈소스 클라우드 네이티브 스토리지 오케스트레이터
  * 가상솔루션을 pod로 배포하여 관리하는 도구
* 스토리지 클래스
  * 관리자가 사용 가능한 다양한 스토리지 유형을 설명할 수 있는 방법을 제공
  * 스토리지클래스 오브젝트는 동적 프로비저닝이 호출 될 때 사용할 프로비저너와 해당 프로비저너에게 전달할 파라미터를 정의

````shell
# 1. git clone
$ git clone --single-branch --branch release-1.5 https://github.com/rook/rook.git

# 2. cd
$ cd ~/rook/cluster/examples/kubernetes/ceph

# --------------------------------------------------------------------------------
# 3. rook operator
$ kubectl create -f crds.yaml -f common.yaml -f operator.yaml

$ kubectl get pod --namespace rook-ceph
NAME                                  READY   STATUS              RESTARTS   AGE
rook-ceph-operator-6db97bfd4c-8hh9v   1/1     Running           0          2m3s

# 4. rook ceph cluster deploy
$ kubectl create -f cluster.yaml
$ kubectl get pod --namespace rook-ceph

# 5. toolbox 테스트
$ kubectl create -f toolbox.yaml
$ kubectl exec -it deploy/rook-ceph-tools --namespace rook-ceph -- bash
$ ceph status
$ kubectl delete -f toolbox.yaml
# --------------------------------------------------------------------------------
# 6. RBD(RADOS Block Device) 블록 스토리지 클래스 생성 (블록 기반 스토리지 제공 기능)
$ kubectl create -f csi/rbd/storageclass.yaml
$ kubectl get storageclasses.storage.k8s.io

# RBD 블록 스토리지 확인
$ kubectl create -f csi/rbd/pod.yaml -f csi/rbd/pvc.yaml
$ kubectl get po,pv,pvc
$ kubectl delete -f csi/rbd/pod.yaml -f csi/rbd/pvc.yaml
# --------------------------------------------------------------------------------
# 7. cephfs 파일시스템 추가
$ kubectl create -f filesystem.yaml
$ kubectl get pod -l app=rook-ceph-mds --namespace rook-ceph

# cephfs 스토리지 클래스 생성
$ kubectl create -f csi/cephfs/storageclass.yaml
$ kubectl get storageclasses.storage.k8s.io

# cephfs 스토리지 확인
$ kubectl create -f csi/cephfs/pod.yaml -f csi/cephfs/pvc.yaml
$ kubectl get po,pv,pvc
$ kubectl delete -f csi/cephfs/pod.yaml -f csi/cephfs/pvc.yam
# --------------------------------------------------------------------------------
# 8. pvc를 생성하여 동적 프로비저닝 사용하기
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: dynamic-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 1Gi
  strorageClassName: rook-cephfs
```
# --------------------------------------------------------------------------------
# 9. 볼륨을 사용할 파드
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: dynamic-pvc-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: dynamic-pvc-rs
  template:
    metadata:
      labels:
        app: dynamic-pvc-rs
    spec:
      containers:
      - name: web-server
        image: nginx:alpine
        volumeMounts:
        - name: web-content
          mountPath: /usr/share/nginx/html
        ports:
        - containerPort: 80
      volumes:
      - name: web-content
        persistentVolumeClaim:
          claimName: dynamic-pvc
````

## 4-3. gitRepo

* https://kubernetes.io/ko/docs/concepts/storage/volumes/#gitrepo
* `gitRepo` 볼륨 유형은 사용 중단되었다. git repo가 있는 컨테이너를 프로비전 하려면 초기화 컨테이너(InitContainer)에 EmptyDir을 마운트하고, 여기에 git을 사용해서 repo를 복제하고, EmptyDir을 파드 컨테이너에 마운트 한다.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: git-pod
spec:
  # 2. initContainers---------------------------
  initContainers:
  - name: git-init
    image: alpine/git
    args:
      - clone
      - --single-branch
      - --
      - https://github.com/kubernetes/kubernetes
      - /repo
    volumeMounts:
    - name: git-volume
      mountPath: /repo
  # 3. containers-------------------------------
  containers:
  - name: git-container
    image: busybox
    args: ['tail', '-f', '/dev/null']
    volumeMounts:
    - name: git-volume
      mountPath: /repo
  # 1. emptyDir ---------------------------------
  volumes:
  - name: git-volume
    emptyDir: {}
```

