---
title:  "Aiffel python 8. Spark RDD"
excerpt:

categories:
  - python

toc: true
toc_sticky: true
 
date: 2021-12-24
---

# RDD 개념

### 등장 배경

하둡 : [파일 -> 디스크에 저장] -> [불러옴 -> 연산(맵리듀스) -> 디스크에 저장] -> 파일처리 작업     
연산마다 디스크 파일을 읽고 쓰기때문에 시간이 너무 오래걸림 -> I/O Bound(주요 병목현상) 발생     
연산단계에서 메모리에 저장하자 -> 메모리는 휘발성 -> 메모리에 적재하기 좋은 추상화 작업 -> RDD

### 정의

탄력적 분산 데이터셋 (RDD: Resilient Distributed Dataset)     
스파크에서 사용하는 기본 추상개념으로 클러스터의 머신(노드)의 여러 메모리에 분산하여 저장할 수 있는 데이터의 집합     
RDD를 스파크라는 프로그램을 통해 실행 시킴으로써 메모리 기반의 대량의 데이터 연산이 가능하게 함

### 특징

In-Memory     
Fault Tolerance     
Immutable(Read-Only) : 결함이 생기는 문제에 대해, 읽기 전용으로 만듬     
Partition : RDD의 전체 데이터 중 부분을 표현하는 단위, Chunck or Partition으로 분할 저장     
Lineage : 데이터를 만드는 방법, 계보를 저장함, 데이터가 유실되면 다시 만드는 방법을 사용      
Lazy evaluation : 느긋한 계산법



# RDD 동작

### 생성 (Creation)

1. `parallelize()`함수 : 내부에서 만들어진 데이터 집합을 병렬화하는 방법
2. `.textFile()` 함수 : 외부의 파일을 로드하는 방법

### 동작 (Operation)

1. Transformations : RDD(데이터셋)를 만듬 (실제 객체는 생성되지 않음, Lineage일 뿐이다.)
    * map, filter, flatMap, sample, groupByKey, reduceByKey, union, join, cogroup, crossProduct, mapValues, sort, partitionBy
2. Actions : 실제 RDD가 생성되는 시점, 연산 수행, 결과값 보여주고 저장
    * count, collect, reduce, lookup, save

### SparkContext

![image](https://user-images.githubusercontent.com/65662520/147357024-1eebc49f-391f-471e-b695-31b2de48566a.png)

SparkContext

* 스파크에서 Drive Program을 구동시킬 때 만드는 객체 (Initializing Spark)
* entry point라고도 함
* 이 객체를 통해 스파크의 모든 기능에 접근
* 스파크 프로그램당 한 번만 실행, 사용 후 종료

### PySpark

PySpark : 스파크에서 제공하는 파이썬 API

```python
from pyspark import SparkConf, SparkContext

sc = SparkContext() # 스파크 프로그램 당 한 번만 실행
sc.stop() # 사용 후 종료
```

```python
sc = SparkContext(master='local', appName='PySpark Basic') # configuration setting 가능
sc.master # 'local'
sc.stop() # 'PySpark Basic'
sc.stop()
```

```python
conf = SparkConf().setAppName('PySpark Basic').setMaster('local') # SparkConf()로 setting
sc = SparkContext(conf=conf)
sc
'''
SparkContext

Spark UI

Version
		v3.0.1
Master
		local
AppName
		PySpark Basic
'''
sc.stop()
```





# RDD 실행

### RDD Creation

1. ``parallelize()`함수 : 내부에서 만들어진 데이터 집합을 병렬화하는 방법

    ```python
    from pyspark import SparkConf, SparkContext
    sc = SparkContext()
    rdd = sc.parallelize([1,2,3])
    ```

    ```python
    rdd          # ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262
    type(rdd)    # pyspark.rdd.RDD
    rdd.take(3)  # [1, 2, 3]
    ```

2. `.textFile()` 함수 : 외부의 파일을 로드하는 방법

    ```python
    from pyspark import SparkConf, SparkContext
    sc = SparkContext()
    rdd2 = sc.textFile(file_path)
    ```

* CSV 파일 적용

    ```python
    import os
    csv_path = os.getenv('HOME')+'/경로'
    csv_data = sc.textFile(csv_path)
    ```

* Spark - Data Frame (Pandas Data Frame과 유사)

    ```python
    from pyspark import SparkConf, SparkContext, SQLContext
    from pyspark import SparkFiles
    
    url = 'https://경로.csv'
    sc.addFile(url)
    sqlContext = SQLContext(sc)
    
    df = sqlContext.read.csv(SparkFiles.get("파일.csv"), header=True, inferSchema= True)
    df.show(5, truncate = False)
    ```

### RDD Operation - Transformation

1. map()

    * x의 모든 원소에 대해 map 함수를 적용한 값은 y다.
    * x와 y의 원소 개수는 같다.

    ```python
    # Transformation
    x = sc.parallelize(["b", "a", "c", "d"])
    y = x.map(lambda z: (z, 1))
    
    # Action: collect() 
    print(x.collect()) # ['b', 'a', 'c', 'd']
    print(y.collect()) # [('b', 1), ('a', 1), ('c', 1), ('d', 1)]
    ```

    ```python
    nums = sc.parallelize([1, 2, 3])
    squares = nums.map(lambda x: x*x)
    print(squares.collect()) # [1, 4, 9]
    ```

2. filter()

    * 조건을 만족하는 값만을 반환
    * x와 y의 개수가 같지 않을 수 있다.

    ```python
    x = sc.parallelize([1,2,3,4,5])
    y = x.filter(lambda x: x%2 == 0) 
    print(y.collect()) # [2, 4]
    ```

3. flatMap()

    * 모든 원소를 단일 원소 스트림으로 반환

    ```python
    x = sc.parallelize([1,2,3])
    y = x.flatMap(lambda x: (x, x*10, 30))
    print(y.collect()) # [1, 10, 30, 2, 20, 30, 3, 30, 30]
    ```

### RDD Operation - Action

1. collect() : RDD 내의 모든 값을 리턴
2. take() : RDD에서 앞쪽 n개의 데이터의 list를 리턴
3. count() : RDD에 포함된 데이터 개수를 리턴
4. reduce() : MapReduce의 Reduce, Spark의 Action 함수, Reduce할 데이터가 RDD로 메모리상에 존재하므로 이전의 다른 구현체보다 훨씬 빠르게 MapReduce를 실행
5. saveAsTextFile() : RDD 데이터를 파일로 저장

```python
nums = sc.parallelize(list(range(10)))  # RDD Creation

nums.collect()                          # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
nums.take(3)                            # [0, 1, 2]
nums.count()                            # 10
nums.reduce(lambda x, y: x + y)         # 45

file_path = os.getenv('HOME')+'/경로/file.txt'
nums.saveAsTextFile(file_path)
```

