---
title:  "DL 03. Artificial Neural Network"
excerpt:

categories:
  - ai

toc: true
toc_sticky: true
 
date: 2021-12-26
---



# 인공신경망(Artificial Neural Network)

범주 : 머신러닝 > 인공신경망 > 딥러닝

인공뉴런 : 인공신경망의 기본단위

뉴런

*  인공신경망 뉴런 : `X_n(정보) ->  W_n(받아들이는 정보) -> Dendrites(수상돌기) -> Cell Body[Summation | Threshold(임계점)] -> Axon(축색돌기)`
* 일반적인 머신러닝 : Summation 까지는 같음
* **인공신경망(Artificial Neural Network) : Threshold - Step function, Nonlinear (복잡한) activation function이 들어감**
* 여러 개의 뉴런들이 연결되어 네트워크 형성

Layer

* Input Units, Hidden Units, Output Units
* Hidden Layer가 2개 이상이면 Deep Learning
* 하나의 Layer를 거치는 것은 하나의 Nonlinear function를 거치는 것이다. -> 엄청 복잡한 함수 (인공지능)을 만들 수 있다.



# 딥러닝을 위한 2가지 문제 해결



### Overfitting

문제 : 학습 데이터에 너무 fitting해서 새로운 데이터가 주어졌을 때는 답을 제대로 맞추지 못하는 것, 과하게 학습(간단한 문제나 데이터가 별로 없는 문제를 복잡하게 모델링), ex) 4차함수

해결 : 복잡한 네트워크를 랜덤하게 뉴런을 끊어 모델을 단순하게 한다.

### Vanishing Gradient

문제

* 딥러닝의 과정에는 미분이 있다(학습되는 양 = 미분값 * 출력값)
* 그런데 step function은 미분을 하면 0이 나와서 Learning 불가 (직선함수 기울기 0)
* step function을 Sigmoid(s자형) 함수로 변형하여 미분가능하게 함 -> 대부분의 영역에서 미분값이 0에 가까움, 출력의 최대값이 1이다. 출력이 최대값이 아니면 0.xxx가 나옴 -> 학습되는 양이 결국은 0에 가깝게 됨

해결

* ReLU 함수 : 미분값이 1, 출력값은 제한 없음
* ReLU를 활성화 함수로 사용





# Deep Learning Process

process : 데이터 준비 → 딥러닝 네트워크 설계 → 학습 → 테스트(평가)

학습단계 : 예측 - 손실(오차) 계산 - 오차역전파

* 오차역전파 : 오차에 대하여 피드백을 통해 파라미터를 조정
* 경사하강법 : 1차 근삿값 발견용 최적화 알고리즘이다. 기본 개념은 함수의 기울기(경사)를 구하고 경사의 절댓값이 낮은 쪽으로 계속 이동시켜 극값에 이를 때까지 반복시키는 것이다.

딥러닝 라이브러리

* TensorFlow : 구글 머신러닝 라이브러리 오픈소스
* tf.keras : 케라스, Sequential API

```python
# 필요한 라이브러리 불러오기
import tensorflow as tf
from tensorflow import keras
print(tf.__version__)   # Tensorflow의 버전 출력
```
