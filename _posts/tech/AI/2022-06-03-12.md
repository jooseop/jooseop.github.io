---
title:  "AI 06. 최적화"
excerpt:

categories:
  - ai

toc: true
toc_sticky: true
 
date: 2022-06-03
---
 
# 하이퍼 파라미터

- 모델 파라미터 : 모델이 학습을 하면서 점차 최적화되는, 그리고 최적화가 되어야 하는 파라미터
- 하이퍼 파라미터 : 모델이 학습을 하기 위해서 사전에 사람이 직접 입력해 주는 파라미터 → 모델 학습 과정에서는 변하지 않음 → 학습 횟수(epoch 수), learning rate (가중치를 업데이트할 학습률), labmda(선형규제) 값


# 최적화

- 하이퍼 파라미터 튜닝
- 그리드탐색 : 사람이 먼저 탐색할 하이퍼 파라미터의 값들을 정해두고, 그 값들로 만들어질 수 있는 모든 조합을 탐색합니다. 특정 값에 대한 하이퍼 파라미터 조합을 모두 탐색하고자 할 때 유리

    ```python
    grid_model = GridSearchCV(model, param_grid=탐색할파라미터(딕셔너리), \
                            scoring=(모델성능평가지표), \
                            cv=tain데이터셋조각수, verbose=메시지출력양, n_jobs=cpu개수)
    ```

    ```python
    param_grid = {
        'n_estimators': [50, 100],
        'max_depth': [1, 10],
    }
    model = LGBMRegressor(random_state=random_state)
    grid_model = GridSearchCV(model, param_grid=param_grid, \
                            scoring='neg_mean_squared_error', \
                            cv=5, verbose=1, n_jobs=5)
    grid_model.fit(train, y)
    ```

- 랜덤탐색 : 사람이 탐색할 하이퍼 파라미터의 공간만 정해두고, 그 안에서 랜덤으로 조합을 선택해서 탐색하는 방법

# 하이퍼파라미터 인자

- lightgbm, xgboost 하이퍼 파라미터 튜닝
- ex, lightgbm 라이브러리 자주 튜닝하는 인자
    - `max_depth` : 의사 결정 나무의 깊이, 정수 사용
    - `learning_rate` : 한 스텝에 이동하는 양을 결정하는 파라미터, 보통 0.0001~0.1 사이의 실수 사용
    - `n_estimators` : 사용하는 개별 모델의 개수, 보통 50~100 이상의 정수 사용
    - `num_leaves` : 하나의 LightGBM 트리가 가질 수 있는 최대 잎의 수
    - `boosting_type` : 부스팅 방식, `gbdt`, `rf` 등의 문자열 입력
