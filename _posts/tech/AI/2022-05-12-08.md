---
title:  "AI 02. 모델"
excerpt:

categories:
  - ai

toc: true
toc_sticky: true
 
date: 2022-05-12
---

> 데이터 준비 : 입력데이터, 정답데이터                             
> 학습 : 모델의 출력인 예측(prediction)      
> 평가 : 정답데이터와 예측데이터를 비교  
---

1. 모델
    - 예측 함수 → 목표
        - 리턴 : 예측값 → 예측값과 정답값의 오차를 줄인다.
        - 함수식 : 선형방정식 → 최적의 직선을 찾는다. (회귀)
    - 선형방정식
        - 모델 → 예측함수 → 오차 최소화 → 최적의 직선
        - `y = w_n * x_n + b` (x 입력데이터, w 가중치, b 상수)
        - 식에서 변경 가능한 것 : w 가중치, b 상수
            
            → 가중치를 결정하여 오차를 줄인다.
            
2. 가중치 결정
    - 오차
        - 예측값과 정답값의 차이
        - 오차 지표 종류
            - RMES : 오차 → 제곱 → 평균 → 제곱근
            - MES : 오차의 절댓값의 평균
            - MAE, R-squared(결정계수)
        - 오차함수
            - 오차를 구하는 함수 (RMES 계산식, MES 계산식, ....)
            - 예측값과 정답값 간의 거리
            - input : 예측값, 정답값
            - output : 오차
    - 가중치
        - 가중치를 결정하여 오차를 줄임 ⇒ 가중치는 오차함수의 기울기(미분)로 판단
        - 경사하강법(Gradient Descent Method)
            - Gradient (기울기)
            - 기울기가 양수면 w를 줄임, 음수면 w를 늘림
            - `w -= ηg`  (η : 학습률, 얼마나 가중치를 업데이트 할 것인가, g : 기울기)
                - g가 양수면 w는 줄어든다.
                - g가 음수면 w는 늘어난다.
                - 하이퍼파라미터 : 사람이 직접 정의할 수 있는 학습률
3. 모델 함수 프로세스

```python
def loss(x, w, b, y):
  예측값 = model(x, w, b)
  오차 = RMSE(예측값, 정답값) # 여러 오차 지표 사용
  return 오차
		
def gradient(x, w, b, y):
  dw, db = loss 미분계산
  return 오차미분값

for i in range(1, 2001):
  # gradient 계산
  dw, db = gradient(x, w, b, y)
  
  # 가중치 업데이트 
  w -= LEARNING_RATE * dw
  b -= LEARNING_RATE * db
	
  # ---------------------------------------
  # 오차 계산 
  L = loss(x, w, b, y)
	
  # 오차 기록
  losses.append(L)
  if i % 100 == 0:
        print('Iteration %d : Loss %0.4f' % (i, L))
```

https://blogs.sas.com/content/saskorea/2017/08/22/최적의-머신러닝-알고리즘을-고르기-위한-치트/

