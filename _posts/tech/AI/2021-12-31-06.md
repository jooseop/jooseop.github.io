---
title:  "DL 06. 딥러닝 수학"
excerpt:

categories:
  - ai

toc: true
toc_sticky: true

use_math: true

date: 2021-12-31
---

# 딥러닝 수학

수학보다 산수, 계산에 가까움

산수는 언어이다. 산수에 쓰이는 단어를 알아가면 됨 (식)

필드에서 대표적으로 쓰는 표기법을 익히기 (Deep Learning Book - Ian Goodfellow)

필요한 수학 분야 : 확률론, 선형대수

# 확률론

### 기댓값 (평균, Expectation)

$$
E[X] = \sum^n_{i=1} x_i * P(x_i)\\
p : 확률 (Probability)
$$
### Continuous 확률변수

(이산이 아닌 연속적) 기댓값
$$
E[x] = \int x \cdot f_x(x) dx
$$
### Probability Density Function (PDF)

면적 = 1

Peak 값은 1보다 클 수 있다.

그 Peak값은 확률이 아니다. -> 면적이 확률이다.

### 확률의 핵심

**Mean = Expectation**

Expectation : 이론적인 평균값

Average : 진짜 sample을 가지고 계산한 평균값

Expectiation과 Average는 다르다.

### 최적화 이론 (Optimization)

오차를 최소화 하는 weight들을 구하기

비용함수 (cost function, loss function)
* loss function : Mean Squared Error, Cross-Entropy

SGD(Stochastic Gradient Descent, 확률적 경사 하강법)

* Mean Squared Error : 오차 제곱의 평균값
    $$
    J(W) = E\{(y_d - y)^2\}
    $$

* SGD(Stochastic Gradient Descent, 확률적 경사 하강법)
    $$
    w(n+1) = w(n) - {1\over 2} \mu {\partial J(w)\over \partial w }
    $$
    
* Vanilla/Batch Gradient Descent : 입력 데이터의 error를 모두 평균내어 하나의 error curve로 생각하자    
    -> Mini-Batch : 모두 평균내지 않고 일정 개수의 데이터 입력만 평균내자



# 선형대수 (Linear Algebra)

### Linearity

Additivity
$$
f(x+y) = f(x) + f(y)
$$
Homogeneity
$$
f(\alpha x) = \alpha f(x)
$$

### 행렬 & 벡터 연산

행렬 계산 방식 그대로 하지 않고, 벡터의 합으로 표현하여 계산    
-> 행렬을 기본이 되는 벡터로 표현할 수 있기 때문에, 기본이 되는 벡터가 어떠한 특징을 가지고 있느냐에 따라서 Ax 의 형태를 분석해 낼 수 있다.





### 독립(Independent)과 직교(Orthogonal)

Rank
* Matrix안에서 Independent한 벡터의 수
* FullRank가 아니면 역행렬이 존재하지 않는다.

Span

* 벡터들로 이루어내는 공간, 차원, 집합

Independent

* 독립
* 다른 벡터들로 만들어 낼 수 없는 관계
* Independent하기만 하면 벡터가 Span하는 공간의 모든 점을 만들어 낼 수 있다. Orthogonal만 기저벡터가 될 수 있는 것은 아니다.
* ICA(Independent Componenet Analysis : 독립성분분석

Orthogonal

* 직교
* PCA(Principal component analysis) : 주성분분석
