---
title:  "DL 05. Recurrent Neural Network"
excerpt:

categories:
  - ai

toc: true
toc_sticky: true
 
use_math: true

date: 2021-12-31
---

### RNN

RNN : Recurrent(반복되는, 되풀이되는) + Neural Network

### Sequence

sequence : 순차적, 입력이나 출력에 시간이 있다.

입력이 한 개가 아닌경우, 입력 또는 출력에 시간 순서가 있는 경우에 RNN을 사용

| 입력   | 출력   | 예시                                                         |
| ------ | ------ | ------------------------------------------------------------ |
| 한 개  | 순차적 | Image Caption Generation<br />이미지 한 장을 넣었는데, 이미지에 대한 언어적 설명이 나오는 경우 |
| 순차적 | 한 개  | Sentiment Classification<br />문장을 넣고, 문장에 대한 감정 구분이 출력으로 나옴<br />음악을 넣고, 음악에 대한 장르를 구분하여 출력<br />감정분석, 주식 가격 예측 |
| 여러개 | 여러개 | Machine Translation<br />기계번역 모델, 개체명 인식 모델     |

### Sequence 학습 방법

문제 : 입력을 순차적으로 모두 넣어준다. -> 입력 사이즈(및 Neural Network 사이즈)가 너무 커지게 된다.

목표 : 입력 사이즈를 유지하면서 과거의 입력 값들을 반영 -> Recurrent Neural Network

Neural Memory(h) : 과거의 값을 넣어주는 방법

* h_t(현재 메모리를 통해 나온 출력값) = h_(t-1)(과거 메모리 출력값) + x_t(현재 메모리 입력값)



### RNN 수학적 이해

아래첨자 : X_in/out
$$
h_t = g(x_t * W_{xh} + W_{hh} * h_{t-1})
$$
W_xh 이후에 Whh*h_(t_1)을 통과시켜주는 것이 non-linear activation function을 포함하고, 뉴럴 네트워크를 너무 복잡하게 통과시키지 않는 방법이다

<img width="320" alt="스크린샷 2021-12-31 오후 12 08 26" src="https://user-images.githubusercontent.com/65662520/147800866-737fd88d-f7b1-4464-8a8f-96a0463fc774.png">
출처 : 쫄지말자 딥러닝 15. RNN 수학적으로 이해하기

### LSTM

Long Short-Term Memory의 배경 :  미분으로 인해 Layer가 쌓일 수록 기울기가 상실되어 출력값을 정확히 도출하지 못한다(Vanishing Gradient). RNN의 Memory도 계속 Layer가 쌓이게 된다. 이것을 해결하기 위해 Long Short-Term Memory가 등장

1. 평균값

$$
C_N = {N-1\over N}{1\over N-1}(\sum^{N-1}_{i=1}x_i)+{1\over N}x_N\\
=\alpha c_{N-1} + (1-\alpha)x_N (0 < \alpha < 1) \\
\alpha = {N-1\over N}, C\space is\space average
$$

2. RNN의 메모리 출력을 이 평균값을 이용한다.
    $$
    C_t = f \cdot C_{t-1} + i \cdot h_t
    $$

    * ht -> ct

    * forget gate와 Input gate는 사람의 수식이 아닌, 또 다른 Neural Network로 학습
        * f : 얼마만큼 이전 값을 forget할 것인가 (Forget Gate)
        * i : 얼마만큼 새로운 값 input을 받아 들일 것인가 (Input Gate)
    * 덧셈을 사용하기 때문에 Vanishing Gradient가 줄어든다.

3. forget gate와 Input gate
    $$
    i = sigmoid(W_{hi}h_{t-1} + W_{xi}x_t)\\
    f = sigmoid(W_{hf}h_{t-1} + W_{xf}x_t)\\
    $$

    * Sigmoid Activation Function은 Vanishing Gradient 문제가 있다. 하지만, 0 < a < 1값을 가지는 가장 좋은 함수이다. 게이트는 알파값이고 이 값을 만족시키는 가장 좋은 함수이다.

4. Output Gate

    * 한번 더 네트워크를 거침
    * 새롭게 만들어진 cell state를 새로운 hidden state에 얼마나 반영할지를 결정하는 gate

    $$
    o = sigmoid(W_{ho}h_{t-1} + W_{xo}x_t)
    $$

5. 최종 메모리 출력
    $$
    h_{t,LSTM}=o \cdot g(c_t)
    $$

### GRU

Gated Recurrent Unit : LSTM의 변형 모델, LSTM의 Forget Gate와 Inpurt Gate를 Update Gate로 합쳤다. 그리고 Cell State와 Hidden State를 합쳤다. LSTM에 비해 GRU가 학습할 가중치(Weight)가 더 적다.
